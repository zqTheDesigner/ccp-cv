{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "established-round",
   "metadata": {},
   "source": [
    "**Feature Detection and Matching:**\n",
    "\n",
    "* Explore feature detection methods such as SIFT or ORB.\n",
    "\t\n",
    "* Implement feature matching to establish correspondence between images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "authentic-platinum",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "181\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "#Load images\n",
    "im1_loc = '/Users/bipulkumar/Desktop/si1.png'\n",
    "im2_loc = '/Users/bipulkumar/Desktop/si2.png'\n",
    "\n",
    "im1 = cv2.imread(im1_loc , cv2.IMREAD_GRAYSCALE)\n",
    "im2 = cv2.imread(im2_loc , cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "#Create a ORB detector\n",
    "orb = cv2.ORB_create()\n",
    "\n",
    "#Find key Points and descriptors\n",
    "keypoints1,descriptors1 = orb.detectAndCompute(im1, None)\n",
    "keypoints2,descriptors2 = orb.detectAndCompute(im2, None)\n",
    "print(len(descriptors1))\n",
    "\n",
    "#Intialise Brute force Matcher\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING , crossCheck=True)\n",
    "\n",
    "#Match Descriptors\n",
    "matches = bf.match(descriptors1,descriptors2)\n",
    "\n",
    "print(len(matches))\n",
    "\n",
    "matching_results = cv2.drawMatches(im1,keypoints1,im2,keypoints2,matches[:15],None,flags=0)\n",
    "\n",
    "cv2.imshow(\"Feature Matching Result\",matching_results)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frank-satisfaction",
   "metadata": {},
   "source": [
    "**cv2.drawMatches(im1,keypoints1,im2,keypoints2,matches[:20],None,flags=2)**\n",
    "\n",
    "* im1 - The first Image (Source Image) on which the key points and matches will be visualised\n",
    "* keypoints1 - List of key points detected in the first image\n",
    "* im2 - The second image (destination image) where matches will be visualised\n",
    "* keypoints2 - List of keypoints detected in second image\n",
    "* matches[:20] - A subset of the matches to be drawn. In this case top 20 matches are selected. We can adjust the number of visualize more or fewer matches\n",
    "* None - Optional mask image. If provided, only the keypoints that are within the mask will be used in the matching visualisation\n",
    "* flags=2 - The flag that specifies how to draw the matches, flags = 2 indicates that the function should draw the matches as lines connecting the key points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aquatic-shipping",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(397, 474)\n",
      "(399, 538)\n",
      "500\n",
      "500\n",
      "500\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "#Load images\n",
    "im1_loc = '/Users/bipulkumar/Desktop/si1.png'\n",
    "im2_loc = '/Users/bipulkumar/Desktop/si2.png'\n",
    "\n",
    "im1 = cv2.imread(im1_loc , cv2.IMREAD_GRAYSCALE)\n",
    "im2 = cv2.imread(im2_loc , cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "#Create a ORB detector\n",
    "orb = cv2.ORB_create()\n",
    "\n",
    "#Find key Points and descriptors\n",
    "keypoints1,descriptors1 = orb.detectAndCompute(im1, None)\n",
    "keypoints2,descriptors2 = orb.detectAndCompute(im2, None)\n",
    "\n",
    "print(im1.shape)\n",
    "print(im2.shape)\n",
    "print(len(keypoints1))\n",
    "print(len(keypoints1))\n",
    "print(len(descriptors1))\n",
    "print(len(descriptors2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "further-airplane",
   "metadata": {},
   "source": [
    "**Activity 5: Image Filtering and Transformation:**\n",
    "* Apply advanced image filtering techniques, such as Gaussian blur and edge detection.\n",
    "* Explore geometric transformations like rotation, scaling, and perspective transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "thousand-venice",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "#Load images\n",
    "im1_loc = '/Users/bipulkumar/Desktop/si1.png'\n",
    "\n",
    "im1 = cv2.imread(im1_loc)\n",
    "\n",
    "#Gaussian Blur\n",
    "blurred_image = cv2.GaussianBlur(im1 , (5,5) , 0)\n",
    "\n",
    "#Canny\n",
    "edges = cv2.Canny(im1 , 100 ,200)\n",
    "\n",
    "#Rotation\n",
    "angle = 30\n",
    "rows,cols = im1.shape[:2]\n",
    "rotation_matrix = cv2.getRotationMatrix2D((cols/2 , rows/2) , angle , 1)\n",
    "rotated_matrix = cv2.warpAffine(im1 , rotation_matrix , (cols,rows))\n",
    "\n",
    "#Scaling\n",
    "scale_factor = 0.5\n",
    "scaled_image = cv2.resize(im1 , None , fx=scale_factor , fy = scale_factor)\n",
    "\n",
    "#Perspective transformation\n",
    "pts1 = np.float32([[50,50] , [200,50] , [50,200] , [200,200]])\n",
    "pts2 = np.float32([[0,0] , [250,0] , [0,250] , [250,250]])\n",
    "\n",
    "perspective_matrix = cv2.getPerspectiveTransform(pts1,pts2)\n",
    "perspective_transformed_image = cv2.warpPerspective(im1, perspective_matrix , (250,250))\n",
    "\n",
    "\n",
    "cv2.imshow(\"Image\",im1)\n",
    "cv2.imshow(\"Blurred\",blurred_image)\n",
    "cv2.imshow(\"Edges\",edges)\n",
    "cv2.imshow(\"Rotation\",rotated_matrix)\n",
    "cv2.imshow(\"Scaled_Image\",scaled_image)\n",
    "cv2.imshow(\"Perspective_Transformed_Image\",perspective_transformed_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disturbed-terminology",
   "metadata": {},
   "source": [
    "**Activity 6: Advanced Computer Vision Techniques:**\n",
    "* Dive into advanced techniques like image segmentation and contour detection.\n",
    "* Implement optical flow for motion analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "right-blind",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "#Load images\n",
    "im1_loc = '/Users/bipulkumar/Desktop/si1.png'\n",
    "\n",
    "im1 = cv2.imread(im1_loc)\n",
    "gray = cv2.cvtColor(im1, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#Gaussian Blur\n",
    "blurred_image = cv2.GaussianBlur(im1 , (5,5) , 0)\n",
    "\n",
    "#Canny\n",
    "edges = cv2.Canny(im1 , 100 ,200)\n",
    "\n",
    "#Identify contours in edge detected images\n",
    "contours, _ = cv2.findContours(edges.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "#print(contours)\n",
    "\n",
    "#Create a copy of the original image for drawing contours\n",
    "contour_image = im1.copy()\n",
    "\n",
    "#Draw and display contours\n",
    "cv2.drawContours(contour_image , contours , -1 , (0,255,0) , 2)\n",
    "\n",
    "#Display original and contour image\n",
    "cv2.imshow(\"Original Image\" , im1)\n",
    "cv2.imshow(\"Contours\" , contour_image)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab4100b-5b98-4f3a-ad3f-6ec6905a4e88",
   "metadata": {},
   "source": [
    "**Implement optical flow for motion analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e590f52c-34c5-4222-8591-6808ef05013e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Using cached opencv_python-4.8.0.76-cp37-abi3-macosx_10_16_x86_64.whl (54.7 MB)\n",
      "Requirement already satisfied: numpy>=1.19.3; python_version >= \"3.9\" in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from opencv-python) (1.25.2)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.8.0.76\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 23.2.1 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.9/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "addressed-professor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "#Capture video feed\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "#Read the frame\n",
    "ret,frame1 = cap.read()\n",
    "prev_gray = cv2.cvtColor(frame1 , cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret,frame2 = cap.read()\n",
    "    current_gray = cv2.cvtColor(frame2 , cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    #Calculate optical flow\n",
    "    flow = cv2.calcOpticalFlowFarneback(prev_gray,current_gray,None,0.5,3,15,3,5,1.2,0)\n",
    "\n",
    "    #Draw motion vectors\n",
    "    motion_image = np.copy(frame2)\n",
    "    for y in range(0, motion_image.shape[0], 10):\n",
    "        for x in range(0, motion_image.shape[1], 10):\n",
    "            dx,dy = flow[y , x]\n",
    "            cv2.arrowedLine(motion_image , (x,y) , (int(x + dx) , int(y + dy)), (0,0,255) , 1)\n",
    "\n",
    "    prev_gray = current_gray\n",
    "\n",
    "    cv2.imshow(\"Video\",frame1)\n",
    "    cv2.imshow(\"Gray Video\",prev_gray)\n",
    "    cv2.imshow(\"Optical Flow\",motion_image)\n",
    "\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86d6787a-f411-45c4-a121-aa7906a155b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord('q')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6a67b9-fd61-465e-89d2-af4120c7ae57",
   "metadata": {},
   "source": [
    "* Create an intuitive user interface (UI) for the application using a GUI library (e.g., Tkinter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2338467e-7106-4038-8e39-50f50083125b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install tkinter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb61b035-bbd7-484e-90a4-dbdc80d994eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from PIL import Image, ImageTk\n",
    "\n",
    "#Create a Tinker GUI\n",
    "root = tk.Tk()\n",
    "root.title(\"OpenCV Image Processing APP\")\n",
    "\n",
    "my_image = None\n",
    "#Function to open a image\n",
    "def open_image():\n",
    "    global my_image\n",
    "    file_path = filedialog.askopenfilename()\n",
    "    if file_path:\n",
    "        image = cv2.imread(file_path)\n",
    "        image = cv2.cvtColor(image , cv2.COLOR_BGR2RGB)\n",
    "        my_image = image\n",
    "        display_image(image)\n",
    "\n",
    "#Function to display image\n",
    "def display_image(image):\n",
    "    pil_image = Image.fromarray(image)\n",
    "    tk_image = ImageTk.PhotoImage(image = pil_image)\n",
    "    label.config(image = tk_image)\n",
    "    label.image = tk_image\n",
    "\n",
    "def convert_to_grayscale():\n",
    "    gray_image = cv2.cvtColor(my_image , cv2.COLOR_BGR2GRAY)\n",
    "    display_image(gray_image)\n",
    "\n",
    "def edge_detect():\n",
    "    edges = cv2.Canny(my_image , 100 , 200)\n",
    "    display_image(edges)\n",
    "\n",
    "def resize():\n",
    "    my_new_image = my_image.copy()\n",
    "    my_new_image = cv2.resize(my_new_image , (200,200))\n",
    "    display_image(my_new_image)\n",
    "\n",
    "def capture_from_camera():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    while cap.isOpened():\n",
    "        ret,frame = cap.read()\n",
    "        display_image(frame)\n",
    "\n",
    "        key = cv2.waitKey(1)\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "        \n",
    "\n",
    "#Create a button \n",
    "open_button = tk.Button(root , text = \"Open Image\" , command = open_image)\n",
    "grayscale_button = tk.Button(root , text = \"Convert to grayScale\" , command = convert_to_grayscale)\n",
    "edge_button = tk.Button(root , text = \"Edge Detection\" , command = edge_detect)\n",
    "resize_button = tk.Button(root , text = \"Resize\" , command = resize)\n",
    "#camera_button = tk.Button(root , text = \"Capture from Camera\" , command = capture_from_camera)\n",
    "\n",
    "#Create an Image display label\n",
    "label = tk.Label(root)\n",
    "label.pack()\n",
    "\n",
    "#Place the button\n",
    "open_button.pack()\n",
    "grayscale_button.pack()\n",
    "edge_button.pack()\n",
    "resize_button.pack()\n",
    "#camera_button.pack()\n",
    "\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3043cb4-8df2-44ba-89e1-24fad4db832d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from PIL import Image, ImageTk\n",
    "\n",
    "# Load YOLO model and COCO class names\n",
    "net = cv2.dnn.readNet('yolov3.weights', 'yolov3.cfg')\n",
    "classes = []\n",
    "with open('coco.names', 'r') as f:\n",
    "    classes = f.read().strip().split('\\n')\n",
    "\n",
    "# Create a Tkinter GUI\n",
    "root = tk.Tk()\n",
    "root.title('YOLO Object Detection App')\n",
    "\n",
    "# Function to open an image using a file dialog\n",
    "def open_image():\n",
    "    file_path = filedialog.askopenfilename()\n",
    "    if file_path:\n",
    "        image = cv2.imread(file_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        detect_objects(image)\n",
    "\n",
    "# Function to display an OpenCV image in the GUI\n",
    "def display_image(cv_image):\n",
    "    pil_image = Image.fromarray(cv_image)\n",
    "    tk_image = ImageTk.PhotoImage(image=pil_image)\n",
    "    label.config(image=tk_image)\n",
    "    label.image = tk_image\n",
    "\n",
    "# Function to perform object detection\n",
    "def detect_objects(cv_image):\n",
    "    height, width = cv_image.shape[:2]\n",
    "\n",
    "    # Prepare input image for YOLO model\n",
    "    blob = cv2.dnn.blobFromImage(cv_image, 1/255, (416, 416), swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "\n",
    "    # Get detection results\n",
    "    outs = net.forward(net.getUnconnectedOutLayersNames())\n",
    "\n",
    "    # Process detection results\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = scores.argmax()\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > 0.7:  # Set a confidence threshold\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "\n",
    "                # Draw bounding box and class label\n",
    "                cv2.rectangle(cv_image, (x, y), (x + w, y + h), (0, 255, 0), 1)\n",
    "                label = f'{classes[class_id]}: {confidence:.2f}'\n",
    "                cv2.putText(cv_image, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "\n",
    "    display_image(cv_image)\n",
    "\n",
    "# Create buttons for opening an image\n",
    "open_button = tk.Button(root, text=\"Open Image\", command=open_image)\n",
    "\n",
    "# Create an image display label\n",
    "label = tk.Label(root)\n",
    "label.pack()\n",
    "\n",
    "# Place buttons in the GUI\n",
    "open_button.pack()\n",
    "\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013e2d2a-e634-495b-b469-3c5a5feabaee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
